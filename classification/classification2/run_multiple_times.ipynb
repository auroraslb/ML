{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom, Resizing, Rescaling\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "X = np.load('Xtrain_Classification_Part2.npy')\n",
    "y = np.load('Ytrain_Classification_Part2.npy')\n",
    "\n",
    "bacc_aug_data = 0\n",
    "bacc_clusters_a_data = 0\n",
    "bacc_clusters = 0\n",
    "bacc_weights_a_aug_data = 0\n",
    "bacc_weights_a_partially_aug_data_african = 0\n",
    "bacc_weights_a_partially_aug_data = 0\n",
    "bacc_weights = 0\n",
    "bacc_cnn = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "\n",
    "    # Split in train and validate \n",
    "    X_train, X_validate, train_labels, test_labels = train_test_split(X,y)\n",
    "\n",
    "\n",
    "    # Reshape to 50x50 pixel images\n",
    "    pixels = 50\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "\n",
    "    for image in X_train:\n",
    "        train_images.append(image.reshape(pixels, pixels))\n",
    "\n",
    "    for image in X_validate:\n",
    "        test_images.append(image.reshape(pixels, pixels))\n",
    "\n",
    "    train_images = np.array(train_images)\n",
    "    test_images = np.array(test_images)\n",
    "\n",
    "\n",
    "    # Normalizing\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "    X_train = X_train / 255.0\n",
    "    X_validate = X_validate / 255.0\n",
    "\n",
    "\n",
    "    # Sort data based on ethnicity\n",
    "    caucasian_count = 0\n",
    "    african_count = 0\n",
    "    asian_count = 0\n",
    "    indian_count = 0\n",
    "\n",
    "    caucasian_train = []\n",
    "    african_train = []\n",
    "    asian_train = []\n",
    "    indian_train = []\n",
    "\n",
    "    for index, label in enumerate(train_labels):\n",
    "        if label == 0:\n",
    "            caucasian_count += 1\n",
    "            caucasian_train.append(train_images[index])\n",
    "        elif label == 1:\n",
    "            african_count += 1\n",
    "            african_train.append(train_images[index])\n",
    "        elif label == 2:\n",
    "            asian_count += 1\n",
    "            asian_train.append(train_images[index])\n",
    "        elif label == 3:\n",
    "            indian_count += 1\n",
    "            indian_train.append(train_images[index])\n",
    "        \n",
    "\n",
    "    # Augment caucasian data by adding mirrored images\n",
    "    caucasian_train = np.array(caucasian_train)\n",
    "    caucasian_augmented = []\n",
    "\n",
    "    caucasian_flipped = caucasian_train[:,:,::-1]\n",
    "\n",
    "    for index, image in enumerate(caucasian_flipped):\n",
    "        caucasian_augmented.append(caucasian_train[index])\n",
    "        caucasian_augmented.append(image)\n",
    "\n",
    "    caucasian_augmented = np.array(caucasian_augmented)\n",
    "\n",
    "\n",
    "    # Augment african data by adding mirrored images\n",
    "    african_train = np.array(african_train)\n",
    "    african_augmented = []\n",
    "\n",
    "    african_flipped = african_train[:,:,::-1]\n",
    "\n",
    "    for index, image in enumerate(african_flipped):\n",
    "        african_augmented.append(african_train[index])\n",
    "        african_augmented.append(image)\n",
    "\n",
    "    african_augmented = np.array(african_augmented)\n",
    "\n",
    "\n",
    "    # Augment asian data by adding mirrored images\n",
    "    asian_train = np.array(asian_train)\n",
    "    asian_augmented = []\n",
    "\n",
    "    asian_flipped = asian_train[:,:,::-1]\n",
    "\n",
    "    for index, image in enumerate(asian_flipped):\n",
    "        asian_augmented.append(asian_train[index])\n",
    "        asian_augmented.append(image)\n",
    "\n",
    "    asian_augmented = np.array(asian_augmented)\n",
    "\n",
    "\n",
    "    # Augment indian data by adding mirrored images\n",
    "    indian_train = np.array(indian_train)\n",
    "    indian_augmented = []\n",
    "\n",
    "    indian_flipped = indian_train[:,:,::-1]\n",
    "\n",
    "    for index, image in enumerate(indian_flipped):\n",
    "        indian_augmented.append(indian_train[index])\n",
    "        indian_augmented.append(image)\n",
    "\n",
    "    indian_augmented = np.array(indian_augmented)\n",
    "\n",
    "\n",
    "    # Create clusters\n",
    "    k = max(african_count, max(asian_count, indian_count))\n",
    "    clusters = KMeans(n_clusters = k, max_iter = 100, n_init = 5, random_state = 20)\n",
    "    caucasian_train = np.array(caucasian_train)\n",
    "    reshaped_data = caucasian_train.reshape(len(caucasian_train),-1)\n",
    "    clusters_caucasian = clusters.fit_predict(reshaped_data)\n",
    "    clusters_caucasian = clusters.cluster_centers_\n",
    "\n",
    "    caucasian_cluster_array = []\n",
    "    for image in clusters_caucasian:\n",
    "        caucasian_cluster_array.append(image.reshape(pixels, pixels))\n",
    "    caucasian_cluster_array = np.array(caucasian_cluster_array)\n",
    "\n",
    "\n",
    "    # Create new training data\n",
    "    train_images_aug_data = []\n",
    "    train_labels_aug_data = []\n",
    "    train_images_clusters_a_data = []\n",
    "    train_labels_clusters_a_data = []\n",
    "    train_images_clusters = []\n",
    "    train_labels_clusters = []\n",
    "    train_images_weights_a_aug_data = []\n",
    "    train_labels_weights_a_aug_data = []\n",
    "    train_images_weights_a_partially_aug_data_african = []\n",
    "    train_labels_weights_a_partially_aug_data_african = []\n",
    "    train_images_weights_a_partially_aug_data = []\n",
    "    train_labels_weights_a_partially_aug_data = []\n",
    "\n",
    "    train_images_weights = train_images\n",
    "    train_labels_weights = train_labels\n",
    "\n",
    "    count_caucasian = 0\n",
    "    count_african = 0\n",
    "    count_asian = 0\n",
    "    count_indian = 0\n",
    "\n",
    "    for index, label in enumerate(train_labels):\n",
    "        if label == 0:\n",
    "            # Aug data\n",
    "            train_images_aug_data.append(caucasian_augmented[count_caucasian].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "            train_images_aug_data.append(caucasian_augmented[count_caucasian + caucasian_count].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "\n",
    "            if count_caucasian < len(caucasian_cluster_array):\n",
    "                # Clusters a data\n",
    "                train_images_clusters_a_data.append(caucasian_cluster_array[caucasian_count].reshape(pixels,pixels))\n",
    "                train_labels_clusters_a_data.append(label)\n",
    "\n",
    "                # Clusters\n",
    "                train_images_clusters.append(caucasian_cluster_array[caucasian_count].reshape(pixels,pixels))\n",
    "                train_labels_clusters.append(label)\n",
    "\n",
    "            # Weights a aug data\n",
    "            train_images_weights_a_aug_data.append(caucasian_augmented[count_caucasian].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "            train_images_weights_a_aug_data.append(caucasian_augmented[count_caucasian + caucasian_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "\n",
    "            # Weights a partially aug data african\n",
    "            train_images_weights_a_partially_aug_data_african.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data_african.append(label)\n",
    "\n",
    "            # Weights a partially aug data\n",
    "            train_images_weights_a_partially_aug_data.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "\n",
    "            count_caucasian += 1\n",
    "\n",
    "        elif label == 1:\n",
    "            # Aug data\n",
    "            train_images_aug_data.append(african_augmented[count_african].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "            train_images_aug_data.append(african_augmented[count_african + african_count].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "\n",
    "            # Clusters a data\n",
    "            train_images_clusters_a_data.append(african_augmented[count_african].reshape(pixels,pixels))\n",
    "            train_labels_clusters_a_data.append(label)\n",
    "            train_images_clusters_a_data.append(african_augmented[count_african + african_count].reshape(pixels,pixels))\n",
    "            train_labels_clusters_a_data.append(label)\n",
    "\n",
    "            # Clusters\n",
    "            train_images_clusters.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_clusters.append(label)\n",
    "\n",
    "            # Weights a aug data\n",
    "            train_images_weights_a_aug_data.append(african_augmented[count_african].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "            train_images_weights_a_aug_data.append(african_augmented[count_african + african_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "\n",
    "            # Weights a partially aug data african\n",
    "            train_images_weights_a_partially_aug_data_african.append(african_augmented[count_african].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data_african.append(label)\n",
    "            train_images_weights_a_partially_aug_data_african.append(african_augmented[count_african + african_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data_african.append(label)\n",
    "\n",
    "            # Weights a partially aug data\n",
    "            train_images_weights_a_partially_aug_data.append(african_augmented[count_african].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "            train_images_weights_a_partially_aug_data.append(african_augmented[count_african + african_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "\n",
    "            count_african += 1\n",
    "\n",
    "        elif label == 2:\n",
    "            # Aug data\n",
    "            train_images_aug_data.append(asian_augmented[count_asian].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "            train_images_aug_data.append(asian_augmented[count_asian + asian_count].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "\n",
    "            # Clusters a data\n",
    "            train_images_clusters_a_data.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_clusters_a_data.append(label)\n",
    "\n",
    "            # Clusters\n",
    "            train_images_clusters.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_clusters.append(label)\n",
    "\n",
    "            # Weights a aug data\n",
    "            train_images_weights_a_aug_data.append(asian_augmented[count_asian].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "            train_images_weights_a_aug_data.append(asian_augmented[count_asian + asian_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "\n",
    "            # Weights a partially aug data african\n",
    "            train_images_weights_a_partially_aug_data_african.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data_african.append(label)\n",
    "\n",
    "            # Weights a partially aug data\n",
    "            train_images_weights_a_partially_aug_data.append(asian_augmented[count_asian].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "            train_images_weights_a_partially_aug_data.append(asian_augmented[count_asian + asian_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "\n",
    "            count_asian += 1\n",
    "\n",
    "        elif label == 3:\n",
    "            # Aug data\n",
    "            train_images_aug_data.append(indian_augmented[count_indian].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "            train_images_aug_data.append(indian_augmented[count_indian + indian_count].reshape(pixels,pixels))\n",
    "            train_labels_aug_data.append(label)\n",
    "\n",
    "            # Clusters a data\n",
    "            train_images_clusters_a_data.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_clusters_a_data.append(label)\n",
    "\n",
    "            # Clusters\n",
    "            train_images_clusters.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_clusters.append(label)\n",
    "\n",
    "            # Weights a aug data\n",
    "            train_images_weights_a_aug_data.append(indian_augmented[count_indian].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "            train_images_weights_a_aug_data.append(indian_augmented[count_indian + indian_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_aug_data.append(label)\n",
    "\n",
    "            # Weights a partially aug data african\n",
    "            train_images_weights_a_partially_aug_data_african.append(train_images[index].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data_african.append(label)\n",
    "\n",
    "            # Weights a partially aug data\n",
    "            train_images_weights_a_partially_aug_data.append(indian_augmented[count_indian].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "            train_images_weights_a_partially_aug_data.append(indian_augmented[count_indian + indian_count].reshape(pixels,pixels))\n",
    "            train_labels_weights_a_partially_aug_data.append(label)\n",
    "\n",
    "            count_indian += 1\n",
    "\n",
    "    # Cast to array\n",
    "    train_images_aug_data = np.array(train_images_aug_data)\n",
    "    train_labels_aug_data = np.array(train_labels_aug_data)\n",
    "    train_images_clusters_a_data = np.array(train_images_clusters_a_data)\n",
    "    train_labels_clusters_a_data = np.array(train_labels_clusters_a_data)\n",
    "    train_images_clusters = np.array(train_images_clusters)\n",
    "    train_labels_clusters = np.array(train_labels_clusters)\n",
    "    train_images_weights_a_aug_data = np.array(train_images_weights_a_aug_data)\n",
    "    train_labels_weights_a_aug_data = np.array(train_labels_weights_a_aug_data)\n",
    "    train_images_weights_a_partially_aug_data_african = np.array(train_images_weights_a_partially_aug_data_african)\n",
    "    train_labels_weights_a_partially_aug_data_african = np.array(train_labels_weights_a_partially_aug_data_african)\n",
    "    train_images_weights_a_partially_aug_data = np.array(train_images_weights_a_partially_aug_data)\n",
    "    train_labels_weights_a_partially_aug_data = np.array(train_labels_weights_a_partially_aug_data)\n",
    "\n",
    "\n",
    "\n",
    "    # Build models and train models\n",
    "\n",
    "    # CNN ---------------------------------------------------------------------------------------------------------\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(  optimizer='adam',\n",
    "                    loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images = np.reshape(train_images, (len(train_images), 50, 50, 1))\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    history = model.fit(train_images, train_labels, validation_data = (test_images, test_labels), epochs = 10)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    bacc_cnn += test_acc\n",
    "\n",
    "    # CNN w aug data ---------------------------------------------------------------------------------------------------------\n",
    "    model_w_aug_data = keras.Sequential()\n",
    "    model_w_aug_data.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_w_aug_data.add(MaxPooling2D((2, 2)))\n",
    "    model_w_aug_data.add(Dropout(0.2))\n",
    "    model_w_aug_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_w_aug_data.add(MaxPooling2D((2, 2)))\n",
    "    model_w_aug_data.add(Dropout(0.2))\n",
    "    model_w_aug_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_w_aug_data.add(Flatten())\n",
    "    model_w_aug_data.add(Dense(64, activation='relu'))\n",
    "    model_w_aug_data.add(Dropout(0.2))\n",
    "    model_w_aug_data.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_w_aug_data.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_aug_data = np.reshape(train_images_aug_data, (len(train_images_aug_data), 50, 50, 1))\n",
    "    train_images_aug_data = np.asarray(train_images_aug_data)\n",
    "    train_labels_aug_data = np.asarray(train_labels_aug_data)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    history = model_w_aug_data.fit(train_images_aug_data, train_labels_aug_data, validation_data = (test_images, test_labels), epochs = 10)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_w_aug_data.evaluate(test_images, test_labels)\n",
    "    bacc_aug_data += test_acc\n",
    "\n",
    "\n",
    "    # CNN w clusters a aug data ---------------------------------------------------------------------------------------------------------\n",
    "    model_clusters_a_data = keras.Sequential()\n",
    "    model_clusters_a_data.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_clusters_a_data.add(MaxPooling2D((2, 2)))\n",
    "    model_clusters_a_data.add(Dropout(0.2))\n",
    "    model_clusters_a_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_clusters_a_data.add(MaxPooling2D((2, 2)))\n",
    "    model_clusters_a_data.add(Dropout(0.2))\n",
    "    model_clusters_a_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_clusters_a_data.add(Flatten())\n",
    "    model_clusters_a_data.add(Dense(64, activation='relu'))\n",
    "    model_clusters_a_data.add(Dropout(0.2))\n",
    "    model_clusters_a_data.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_clusters_a_data.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_clusters_a_data = np.reshape(train_images_clusters_a_data, (len(train_images_clusters_a_data), 50, 50, 1))\n",
    "    train_images_clusters_a_data = np.asarray(train_images_clusters_a_data)\n",
    "    train_labels_clusters_a_data = np.asarray(train_labels_clusters_a_data)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    history = model_clusters_a_data.fit(train_images_clusters_a_data, train_labels_clusters_a_data, validation_data = (test_images, test_labels), epochs = 10)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_clusters_a_data.evaluate(test_images, test_labels)\n",
    "    bacc_clusters_a_data += test_acc\n",
    "\n",
    "\n",
    "    # CNN w clusters ---------------------------------------------------------------------------------------------------------\n",
    "    model_clusters = keras.Sequential()\n",
    "    model_clusters.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_clusters.add(MaxPooling2D((2, 2)))\n",
    "    model_clusters.add(Dropout(0.2))\n",
    "    model_clusters.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_clusters.add(MaxPooling2D((2, 2)))\n",
    "    model_clusters.add(Dropout(0.2))\n",
    "    model_clusters.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_clusters.add(Flatten())\n",
    "    model_clusters.add(Dense(64, activation='relu'))\n",
    "    model_clusters.add(Dropout(0.2))\n",
    "    model_clusters.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_clusters.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_clusters = np.reshape(train_images_clusters, (len(train_images_clusters), 50, 50, 1))\n",
    "    train_images_clusters = np.asarray(train_images_clusters)\n",
    "    train_labels_clusters = np.asarray(train_labels_clusters)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    history = model_clusters.fit(train_images_clusters, train_labels_clusters, validation_data = (test_images, test_labels), epochs = 10)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_clusters.evaluate(test_images, test_labels)\n",
    "    bacc_clusters += test_acc\n",
    "\n",
    "\n",
    "    # CNN w weights a aug data ---------------------------------------------------------------------------------------------------------\n",
    "    model_weights_a_aug_data = keras.Sequential()\n",
    "    model_weights_a_aug_data.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_weights_a_aug_data.add(MaxPooling2D((2, 2)))\n",
    "    model_weights_a_aug_data.add(Dropout(0.2))\n",
    "    model_weights_a_aug_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights_a_aug_data.add(MaxPooling2D((2, 2)))\n",
    "    model_weights_a_aug_data.add(Dropout(0.2))\n",
    "    model_weights_a_aug_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights_a_aug_data.add(Flatten())\n",
    "    model_weights_a_aug_data.add(Dense(64, activation='relu'))\n",
    "    model_weights_a_aug_data.add(Dropout(0.2))\n",
    "    model_weights_a_aug_data.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_weights_a_aug_data.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_weights_a_aug_data = np.reshape(train_images_weights_a_aug_data, (len(train_images_weights_a_aug_data), 50, 50, 1))\n",
    "    train_images_weights_a_aug_data = np.asarray(train_images_weights_a_aug_data)\n",
    "    train_labels_weights_a_aug_data = np.asarray(train_labels_weights_a_aug_data)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    # Calculate weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels_weights_a_aug_data), train_labels_weights_a_aug_data)\n",
    "    classes = [0, 1, 2, 3]\n",
    "    dict_weights = dict(zip(classes, class_weights.T))\n",
    "\n",
    "    history = model_weights_a_aug_data.fit(train_images_weights_a_aug_data, train_labels_weights_a_aug_data, validation_data = (test_images, test_labels), epochs = 10, class_weight=dict_weights)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_weights_a_aug_data.evaluate(test_images, test_labels)\n",
    "    bacc_weights_a_aug_data += test_acc\n",
    "\n",
    "\n",
    "    # CNN w weights a partially aug data african ---------------------------------------------------------------------------------------------------------\n",
    "    model_weights_a_partially_aug_data_african = keras.Sequential()\n",
    "    model_weights_a_partially_aug_data_african.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_weights_a_partially_aug_data_african.add(MaxPooling2D((2, 2)))\n",
    "    model_weights_a_partially_aug_data_african.add(Dropout(0.2))\n",
    "    model_weights_a_partially_aug_data_african.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights_a_partially_aug_data_african.add(MaxPooling2D((2, 2)))\n",
    "    model_weights_a_partially_aug_data_african.add(Dropout(0.2))\n",
    "    model_weights_a_partially_aug_data_african.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights_a_partially_aug_data_african.add(Flatten())\n",
    "    model_weights_a_partially_aug_data_african.add(Dense(64, activation='relu'))\n",
    "    model_weights_a_partially_aug_data_african.add(Dropout(0.2))\n",
    "    model_weights_a_partially_aug_data_african.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_weights_a_partially_aug_data_african.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_weights_a_partially_aug_data_african = np.reshape(train_images_weights_a_partially_aug_data_african, (len(train_images_weights_a_partially_aug_data_african), 50, 50, 1))\n",
    "    train_images_weights_a_partially_aug_data_african = np.asarray(train_images_weights_a_partially_aug_data_african)\n",
    "    train_labels_weights_a_partially_aug_data_african = np.asarray(train_labels_weights_a_partially_aug_data_african)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    # Calculate weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels_weights_a_partially_aug_data_african), train_labels_weights_a_partially_aug_data_african)\n",
    "    classes = [0, 1, 2, 3]\n",
    "    dict_weights = dict(zip(classes, class_weights.T))\n",
    "\n",
    "    history = model_weights_a_partially_aug_data_african.fit(train_images_weights_a_partially_aug_data_african, train_labels_weights_a_partially_aug_data_african, validation_data = (test_images, test_labels), epochs = 10, class_weight=dict_weights)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_weights_a_partially_aug_data_african.evaluate(test_images, test_labels)\n",
    "    bacc_weights_a_partially_aug_data_african += test_acc\n",
    "\n",
    "\n",
    "    # CNN w weights a partially aug data ---------------------------------------------------------------------------------------------------------\n",
    "    model_weights_a_partially_aug_data = keras.Sequential()\n",
    "    model_weights_a_partially_aug_data.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_weights_a_partially_aug_data.add(MaxPooling2D((2, 2)))\n",
    "    model_weights_a_partially_aug_data.add(Dropout(0.2))\n",
    "    model_weights_a_partially_aug_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights_a_partially_aug_data.add(MaxPooling2D((2, 2)))\n",
    "    model_weights_a_partially_aug_data.add(Dropout(0.2))\n",
    "    model_weights_a_partially_aug_data.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights_a_partially_aug_data.add(Flatten())\n",
    "    model_weights_a_partially_aug_data.add(Dense(64, activation='relu'))\n",
    "    model_weights_a_partially_aug_data.add(Dropout(0.2))\n",
    "    model_weights_a_partially_aug_data.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_weights_a_partially_aug_data.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_weights_a_partially_aug_data = np.reshape(train_images_weights_a_partially_aug_data, (len(train_images_weights_a_partially_aug_data), 50, 50, 1))\n",
    "    train_images_weights_a_partially_aug_data = np.asarray(train_images_weights_a_partially_aug_data)\n",
    "    train_labels_weights_a_partially_aug_data = np.asarray(train_labels_weights_a_partially_aug_data)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    # Calculate weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels_weights_a_partially_aug_data), train_labels_weights_a_partially_aug_data)\n",
    "    classes = [0, 1, 2, 3]\n",
    "    dict_weights = dict(zip(classes, class_weights.T))\n",
    "\n",
    "    history = model_weights_a_partially_aug_data.fit(train_images_weights_a_partially_aug_data, train_labels_weights_a_partially_aug_data, validation_data = (test_images, test_labels), epochs = 10, class_weight=dict_weights)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_weights_a_partially_aug_data.evaluate(test_images, test_labels)\n",
    "    bacc_weights_a_partially_aug_data += test_acc\n",
    "\n",
    "\n",
    "    # CNN w weights ---------------------------------------------------------------------------------------------------------\n",
    "    model_weights = keras.Sequential()\n",
    "    model_weights.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(50, 50, 1)))\n",
    "    model_weights.add(MaxPooling2D((2, 2)))\n",
    "    model_weights.add(Dropout(0.2))\n",
    "    model_weights.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights.add(MaxPooling2D((2, 2)))\n",
    "    model_weights.add(Dropout(0.2))\n",
    "    model_weights.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model_weights.add(Flatten())\n",
    "    model_weights.add(Dense(64, activation='relu'))\n",
    "    model_weights.add(Dropout(0.2))\n",
    "    model_weights.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model_weights.compile(  optimizer='adam',\n",
    "                                loss = 'sparse_categorical_crossentropy', #'binary_crossentropy',\n",
    "                                metrics = ['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    train_images_weights = np.reshape(train_images_weights, (len(train_images_weights), 50, 50, 1))\n",
    "    train_images_weights = np.asarray(train_images_weights)\n",
    "    train_labels_weights = np.asarray(train_labels_weights)\n",
    "\n",
    "    test_images = np.reshape(test_images, (len(test_images),50, 50, 1))\n",
    "\n",
    "    # Calculate weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels_weights), train_labels_weights)\n",
    "    classes = [0, 1, 2, 3]\n",
    "    dict_weights = dict(zip(classes, class_weights.T))\n",
    "\n",
    "    history = model_weights.fit(train_images_weights, train_labels_weights, validation_data = (test_images, test_labels), epochs = 10, class_weight=dict_weights)\n",
    "\n",
    "    test_images = test_images.reshape(-1, 50, 50, 1)\n",
    "    test_loss, test_acc = model_weights.evaluate(test_images, test_labels)\n",
    "    bacc_weights += test_acc\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
